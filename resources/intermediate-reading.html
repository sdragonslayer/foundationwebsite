---
layout: default
---
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Safety Foundations - Intermediate Reading List</title>
    <link rel="stylesheet" href="../assets/css/styles.css">
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-G-F605BZW00S"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-F605BZW00S');
    </script>
</head>
<body>

    <main>
        <section class="hero">
            <div class="container hero-content">
                <h1>AI Safety Intermediate Reading List</h1>
                <p>A selection of more in-depth books, research papers, and technical blogs for those with some foundational AI safety knowledge.</p>
            </div>
        </section>

        <div class="container">
            <section>
                <h2>Diving Deeper into AI Safety</h2>
                <p>This reading list is intended for individuals who have a basic understanding of AI safety concepts (perhaps from our <a href="beginner-reading">Beginner Reading List</a>) and are looking to explore more technical, nuanced, or advanced topics. Some resources may require familiarity with machine learning concepts.</p>
            </section>

            <section class="reading-list">
                <h2>Key Research Papers & Technical Blogs</h2>
                <p>Many of these can be found on <a href="https://arxiv.org/" target="_blank">arXiv</a> (search for relevant keywords like "AI alignment," "interpretability," "robustness") or on the websites of AI research labs (OpenAI, DeepMind, Anthropic, etc.).</p>

                <div class="card">
                    <div class="card-content">
                        <h3>Concrete Problems in AI Safety (Amodei et al., 2016)</h3>
                        <p><strong>Why read it:</strong> One of the seminal papers outlining specific, practical research problems in AI safety, such as avoiding negative side effects, safe exploration, and reward hacking. Still highly relevant.</p>
                        <p><strong>Good for:</strong> Understanding a research agenda for making AI systems safer in the near-to-medium term.</p>
                        <p><strong>Link:</strong> <a href="https://arxiv.org/abs/1606.06565" target="_blank">arXiv:1606.06565</a></p>
                    </div>
                </div>

                <div class="card">
                    <div class="card-content">
                        <h3>The Case for Aligning AGI with Human Norms (Gabriel, 2020)</h3>
                        <p><strong>Why read it:</strong> Argues for aligning AI not just with explicit instructions but with broader human social and ethical norms. Discusses the philosophical underpinnings of value alignment.</p>
                        <p><strong>Good for:</strong> Thinking about the ethical dimensions of AI alignment and the role of implicit knowledge.</p>
                        <p><strong>Link:</strong> <a href="https://arxiv.org/abs/2007.03780" target="_blank">arXiv:2007.03780</a> (or search for published version)</p>
                    </div>
                </div>

                <div class="card">
                    <div class="card-content">
                        <h3>Unrestricted Adversarial Examples (various authors)</h3>
                        <p><strong>Why read it:</strong> Research on adversarial examples has evolved. Understanding "unrestricted" or "semantic" attacks (which are perceptible to humans but still fool models) is important for robust AI.</p>
                        <p><strong>Good for:</strong> Deepening knowledge of AI model vulnerabilities beyond simple pixel changes.</p>
                        <p><em>Suggestion: Search for recent papers on "semantic adversarial attacks" or "natural adversarial examples."</em></p>
                    </div>
                </div>

                <div class="card">
                    <div class="card-content">
                        <h3>Distill.pub Articles</h3>
                        <p><strong>Why read it:</strong> Distill was a journal focused on clear explanations of machine learning research. Many articles are highly visual and interactive, covering topics like interpretability, attention mechanisms, and model vulnerabilities.</p>
                        <p><strong>Good for:</strong> Gaining intuitive understanding of complex ML concepts relevant to safety.</p>
                        <p><strong>Link:</strong> <a href="https://distill.pub/" target="_blank">Distill.pub</a> (Note: no longer active but archive is valuable)</p>
                    </div>
                </div>

                <div class="card">
                    <div class="card-content">
                        <h3>AI Impacts Blog & Research</h3>
                        <p><strong>Why explore it:</strong> Provides detailed, evidence-based estimates and discussions about various aspects of AI development, timelines, and potential impacts, including safety considerations.</p>
                        <p><strong>Good for:</strong> A more quantitative and analytical perspective on AI futures and risks.</p>
                        <p><strong>Link:</strong> <a href="https://aiimpacts.org/" target="_blank">AI Impacts</a></p>
                    </div>
                </div>

                 <div class="card">
                    <div class="card-content">
                        <h3>Research from Major AI Labs</h3>
                        <p><strong>Why explore it:</strong> Regularly check the publications sections of labs like OpenAI, DeepMind, Anthropic, Google AI, Meta AI, and leading university labs (e.g., CHAI at Berkeley, Stanford HAI).</p>
                        <p><strong>Good for:</strong> Staying on the cutting edge of technical AI safety and alignment research.</p>
                        <p><em>Example areas: Scalable oversight, mechanistic interpretability, learned optimization, adversarial robustness.</em></p>
                    </div>
                </div>
            </section>

            <section class="reading-list">
                <h2>More Advanced Books</h2>
                <div class="card">
                    <div class="card-content">
                        <h3><em>The Precipice: Existential Risk and the Future of Humanity</em> by Toby Ord</h3>
                        <p><strong>Why read it:</strong> While not solely about AI, it provides a rigorous framework for thinking about existential risks, with a significant portion dedicated to risks from unaligned AGI. Complements "Superintelligence" by focusing on the broader risk landscape.</p>
                        <p><strong>Good for:</strong> Understanding the severity and probability of various global catastrophic risks, and AI's place among them.</p>
                    </div>
                </div>
                <div class="card">
                    <div class="card-content">
                        <h3><em>Probabilistic Machine Learning: An Introduction / Advanced Topics</em> by Kevin Murphy</h3>
                        <p><strong>Why read it:</strong> For those wanting to dive deep into the technical underpinnings of modern AI, these textbooks are comprehensive. Understanding ML deeply is crucial for much of technical AI safety research.</p>
                        <p><strong>Good for:</strong> Aspiring technical researchers; a strong mathematical background is helpful.</p>
                    </div>
                </div>
            </section>

            <section class="reading-list">
                <h2>Technical Blogs and Forums</h2>
                <div class="card">
                    <div class="card-content">
                        <h3>Chris Olah's Blog (and related work on interpretability)</h3>
                        <p><strong>Why read it:</strong> Known for groundbreaking work in visualizing and understanding neural networks. Essential reading for anyone interested in mechanistic interpretability.</p>
                        <p><strong>Link:</strong> <a href="https://colah.github.io/" target="_blank">colah.github.io</a></p>
                    </div>
                </div>
                <div class="card">
                    <div class="card-content">
                        <h3>The Alignment Forum (Technical Posts)</h3>
                        <p><strong>Why explore it:</strong> Beyond introductory material, the Alignment Forum hosts many technical discussions, proposals, and critiques of AI alignment research. Filter for more technical tags or authors.</p>
                        <p><strong>Link:</strong> <a href="https://www.alignmentforum.org" target="_blank">Alignment Forum</a></p>
                    </div>
                </div>
            </section>

            <section>
                <h2>Considerations for Intermediate Learners</h2>
                <ul>
                    <li><strong>Identify Specializations:</strong> AI safety is broad. Consider focusing on sub-fields like interpretability, robustness, value learning, governance, etc.</li>
                    <li><strong>Engage with Critiques:</strong> Seek out thoughtful criticisms of existing approaches to deepen your understanding.</li>
                    <li><strong>Mathematical Maturity:</strong> Some areas (like formal verification or advanced ML theory) require a solid grasp of mathematics (linear algebra, probability, calculus).</li>
                    <li><strong>Practical Experience:</strong> If possible, try to implement or experiment with some of the concepts you're learning about (e.g., training small models, trying interpretability techniques).</li>
                </ul>
            </section>

            <section>
                <h2>Continue Your Journey</h2>
                <p>The field is constantly evolving. Stay curious, keep learning, and consider how you can contribute. Check our <a href="../learn/research">Research Breakdowns</a> for summaries of newer papers.</p>
            </section>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 AI Safety Foundations. All rights reserved.</p>
            <div class="footer-links">
                <a href="../pages/about">About</a>
                <a href="../pages/learn">Learn</a>
                <a href="../pages/resources">Resources</a>
            </div>
        </div>
    </footer>
</body>
</html>
