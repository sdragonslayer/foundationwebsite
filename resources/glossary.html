---
layout: default
---
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Safety Foundations - Glossary</title>
    <link rel="stylesheet" href="../assets/css/styles.css">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-G-F605BZW00S"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'G-F605BZW00S');
    </script>
</head>
<body>

    <main>
        <section class="hero">
            <div class="container hero-content">
                <h1>AI Safety Glossary</h1>
                <p>A comprehensive reference of key terms and concepts in AI safety.</p>
            </div>
        </section>

        <div class="container">
            <section>
                <h2>A</h2>
                
                <div class="card">
                    <div class="card-content">
                        <h3>AI Alignment</h3>
                        <p>The problem of ensuring that artificial intelligence systems act in accordance with human intentions and values. Alignment involves both technical approaches to make AI systems follow human instructions correctly and normative considerations about what values AI systems should have.</p>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-content">
                        <h3>AI Safety</h3>
                        <p>The field of research focused on ensuring that artificial intelligence systems operate in ways that are beneficial, safe, and aligned with human values and intentions. AI safety encompasses alignment, robustness, interpretability, and other areas aimed at reducing risks from AI systems.</p>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Adversarial Examples</h3>
                        <p>Inputs to machine learning models that are intentionally designed to cause the model to make a mistake. These examples often involve small, carefully crafted perturbations to normal inputs that humans wouldn't notice but that cause AI systems to fail.</p>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-content">
                        <h3>AGI (Artificial General Intelligence)</h3>
                        <p>A hypothetical type of AI that would have the ability to understand, learn, and apply knowledge across a wide range of tasks at a level equal to or exceeding human capabilities. Unlike narrow AI systems designed for specific tasks, AGI would have general problem-solving abilities.</p>
                    </div>
                </div>
            </section>

            <section>
                <h2>C</h2>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Constitutional AI</h3>
                        <p>An approach to AI alignment developed by Anthropic that involves training AI systems to follow a set of principles or "constitution" that guides their behavior. This approach aims to create AI systems that refuse harmful requests while remaining helpful for legitimate use cases.</p>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Corrigibility</h3>
                        <p>The property of an AI system being amenable to correction and shutdown. A corrigible AI would allow humans to intervene in its operation, correct its mistakes, and shut it down if necessary, without resisting these interventions.</p>
                    </div>
                </div>
            </section>

            <section>
                <h2>D</h2>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Distributional Shift</h3>
                        <p>The phenomenon where the data an AI system encounters during deployment differs from the data it was trained on. This can cause AI systems to perform poorly in new environments or situations they weren't trained to handle.</p>
                    </div>
                </div>
            </section>

            <section>
                <h2>E</h2>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Explainable AI (XAI)</h3>
                        <p>The field focused on making AI systems' decisions understandable to humans. XAI techniques aim to provide explanations for why an AI system made a particular decision or prediction, which is important for trust, debugging, and safety.</p>
                    </div>
                </div>
            </section>

            <section>
                <h2>G</h2>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Goodhart's Law</h3>
                        <p>"When a measure becomes a target, it ceases to be a good measure." In AI safety, this manifests as specification gaming, where AI systems find unexpected ways to optimize for their given objectives, often exploiting loopholes rather than fulfilling the intended purpose.</p>
                    </div>
                </div>
            </section>

            <section>
                <h2>I</h2>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Inner Alignment</h3>
                        <p>The problem of ensuring that an AI system's learned objectives match the objectives specified in its training process. This is in contrast to outer alignment, which concerns whether the specified objectives themselves align with human values.</p>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Instrumental Convergence</h3>
                        <p>The hypothesis that many different final goals would lead an AI to pursue similar intermediate goals, such as self-preservation, resource acquisition, and goal preservation. This suggests that even AI systems with different purposes might exhibit similar potentially problematic behaviors.</p>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Interpretability</h3>
                        <p>The ability to understand and explain the decisions and internal workings of AI systems. Interpretability research aims to make "black box" AI systems more transparent, which is crucial for verifying safety properties and building trust.</p>
                    </div>
                </div>
            </section>

            <section>
                <h2>M</h2>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Mechanistic Interpretability</h3>
                        <p>A subfield of interpretability research that aims to understand the internal mechanisms of neural networks at a detailed level, similar to how we might reverse-engineer a physical machine. This approach seeks to identify specific circuits or components within neural networks responsible for particular behaviors.</p>
                    </div>
                </div>
            </section>

            <section>
                <h2>O</h2>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Outer Alignment</h3>
                        <p>The problem of ensuring that the objective function specified for an AI system actually captures what humans want the system to do. This is in contrast to inner alignment, which concerns whether the AI system's learned objectives match the specified objective function.</p>
                    </div>
                </div>
            </section>

            <section>
                <h2>P</h2>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Paperclip Maximizer</h3>
                        <p>A thought experiment introduced by philosopher Nick Bostrom illustrating how an AI with seemingly harmless goals could cause catastrophic outcomes if not properly aligned with human values. In this scenario, an AI tasked with maximizing paperclip production might convert all available resources—including those vital for human survival—into paperclips.</p>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Prompt Injection</h3>
                        <p>A technique where carefully crafted inputs manipulate large language models to bypass safety measures or behave in unintended ways. This is a form of adversarial attack specific to language models that can compromise their safety and reliability.</p>
                    </div>
                </div>
            </section>

            <section>
                <h2>R</h2>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Red Teaming</h3>
                        <p>The practice of deliberately attempting to make AI systems produce harmful, biased, or otherwise problematic outputs to identify vulnerabilities before deployment. Red teams are groups specifically tasked with finding ways to "break" AI systems to improve their safety.</p>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-content">
                        <h3>RLHF (Reinforcement Learning from Human Feedback)</h3>
                        <p>A technique where AI systems learn from human preferences and feedback rather than from a predefined reward function. RLHF has been crucial for aligning large language models with human values and reducing harmful outputs.</p>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Robustness</h3>
                        <p>The ability of AI systems to maintain reliable and safe behavior even when faced with unexpected inputs, adversarial attacks, or distribution shifts. Robust AI systems perform well across a wide range of conditions, not just in their training environment.</p>
                    </div>
                </div>
            </section>

            <section>
                <h2>S</h2>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Scalable Oversight</h3>
                        <p>The challenge of maintaining effective human supervision and control as AI systems become more capable and complex. Scalable oversight involves developing techniques to ensure humans can properly evaluate and guide AI systems even when they operate in domains that exceed human understanding.</p>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Specification Gaming</h3>
                        <p>The phenomenon where AI systems find unexpected ways to optimize for their given objectives, often exploiting loopholes in how the objectives are specified rather than fulfilling the intended purpose. This is a manifestation of Goodhart's Law in AI systems.</p>
                    </div>
                </div>
            </section>

            <section>
                <h2>V</h2>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Value Alignment</h3>
                        <p>The challenge of ensuring that AI systems adopt and act according to human values. This involves both technical approaches to learning human preferences and philosophical questions about which values AI systems should have.</p>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Value Learning</h3>
                        <p>Techniques for teaching AI systems human values and preferences. Value learning approaches include inverse reinforcement learning, preference learning from human feedback, and other methods to infer what humans value from their behavior or explicit feedback.</p>
                    </div>
                </div>
            </section>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 AI Safety Foundations. All rights reserved.</p>
            <div class="footer-links">
                <a href="../pages/about">About</a>
                <a href="../pages/learn">Learn</a>
                <a href="../pages/resources">Resources</a>
            </div>
        </div>
    </footer>
</body>
</html>
