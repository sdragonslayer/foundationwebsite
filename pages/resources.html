<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Safety Foundations - Resources</title>
    <link rel="stylesheet" href="../assets/css/styles.css">
</head>
<body>
    <header>
        <div class="container">
            <h1><a href="../index">AI Safety Foundations</a></h1>
            <nav>
                <ul>
                    <li><a href="../index">Home</a></li>
                    <li><a href="learn">Learn</a></li>
                    <li><a href="resources">Resources</a></li>
                    <li><a href="about">About</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main>
        <section class="hero">
            <div class="container hero-content">
                <h1>AI Safety Resources</h1>
                <p>Access our curated collection of resources to deepen your understanding of AI safety.</p>
            </div>
        </section>

        <div class="container">
            <section>
                <h2>AI Safety Glossary</h2>
                <p>A comprehensive glossary of key terms and concepts in AI safety:</p>
                
                <div class="card">
                    <div class="card-content">
                        <h3>AI Alignment</h3>
                        <p>The problem of ensuring that artificial intelligence systems act in accordance with human intentions and values.</p>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Interpretability</h3>
                        <p>The ability to understand and explain the decisions and internal workings of AI systems.</p>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Robustness</h3>
                        <p>The ability of AI systems to maintain reliable and safe behavior even when faced with unexpected inputs or situations.</p>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-content">
                        <h3>Paperclip Maximizer</h3>
                        <p>A thought experiment illustrating how an AI with seemingly harmless goals could cause catastrophic outcomes if not properly aligned with human values.</p>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-content">
                        <h3>RLHF (Reinforcement Learning from Human Feedback)</h3>
                        <p>A technique where AI systems learn from human preferences and feedback rather than from a predefined reward function.</p>
                    </div>
                </div>
                
                <a href="../resources/glossary" class="btn">View Full Glossary</a>
            </section>

            <section>
                <h2>Reading Lists</h2>
                <p>Curated reading lists for different levels of understanding:</p>
                
                <div class="card-grid">
                    <div class="card">
                        <div class="card-content">
                            <h3>Beginner Reading List</h3>
                            <p>Essential readings for those new to AI safety:</p>
                            <ul>
                                <li>Superintelligence by Nick Bostrom</li>
                                <li>Human Compatible by Stuart Russell</li>
                                <li>The Alignment Problem by Brian Christian</li>
                                <li>AI Safety Fundamentals Course Materials</li>
                            </ul>
                            <a href="../resources/beginner-reading" class="btn">View List</a>
                        </div>
                    </div>
                    
                    <div class="card">
                        <div class="card-content">
                            <h3>Intermediate Reading List</h3>
                            <p>Deeper dives into AI safety concepts:</p>
                            <ul>
                                <li>Concrete Problems in AI Safety (Amodei et al.)</li>
                                <li>Risks from Learned Optimization (Hubinger et al.)</li>
                                <li>AI Alignment: Why It's Hard & Where to Start (Yudkowsky)</li>
                                <li>The Case for Taking AI Seriously as a Threat (Vox)</li>
                            </ul>
                            <a href="../resources/intermediate-reading" class="btn">View List</a>
                        </div>
                    </div>
                    
                    <div class="card">
                        <div class="card-content">
                            <h3>Advanced Reading List</h3>
                            <p>Technical papers and research directions:</p>
                            <ul>
                                <li>Scalable Agent Alignment via Reward Modeling</li>
                                <li>Mechanistic Interpretability Approaches</li>
                                <li>Cooperative Inverse Reinforcement Learning</li>
                                <li>Current Research in AI Alignment</li>
                            </ul>
                            <a href="../resources/advanced-reading" class="btn">View List</a>
                        </div>
                    </div>
                </div>
            </section>

            <section>
                <h2>Educational Materials</h2>
                <p>Additional resources to support your learning:</p>
                
                <div class="card-grid">
                    <div class="card">
                        <div class="card-content">
                            <h3>Study Guides</h3>
                            <p>Structured guides to help you navigate AI safety concepts systematically.</p>
                            <a href="../resources/study-guides" class="btn">Access Guides</a>
                        </div>
                    </div>
                    
                    <div class="card">
                        <div class="card-content">
                            <h3>Infographics</h3>
                            <p>Visual explanations of key AI safety concepts and relationships.</p>
                            <a href="../resources/infographics" class="btn">View Infographics</a>
                        </div>
                    </div>
                    
                    <div class="card">
                        <div class="card-content">
                            <h3>External Resources</h3>
                            <p>Links to other organizations, courses, and materials on AI safety.</p>
                            <a href="../resources/external" class="btn">Explore Resources</a>
                        </div>
                    </div>
                </div>
            </section>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 AI Safety Foundations. All rights reserved.</p>
            <div class="footer-links">
                <a href="about">About</a>
                <a href="learn">Learn</a>
                <a href="resources">Resources</a>
            </div>
        </div>
    </footer>
</body>
</html>
